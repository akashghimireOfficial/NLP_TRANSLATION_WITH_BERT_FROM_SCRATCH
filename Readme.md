# NLP Machine Translation from Scratch

In this tutorial, I trained an NLP Machine Translation model using the ``BERT`` architecture from *scratch*. The motivation behind this tutorial is to gain a deeper understanding of the components within the transformer.

I've also written a note about how the attention mechanism in transformer architecture operates, which can be found in [this_readme_file](transformer_from_scratch_turtorial/understanding_transformer.md). In this note, I elucidated what attention is in general, providing straightforward examples. Concepts like query, key, and value are explained comprehensively. Moreover, the functioning of `Multi-Head Attention` within the Encoder and Decoder is delved into in greater detail in this document. Additional crucial concepts are also elaborated on in the main Jupyter notebook file.

For a complete overview of the Translation Process from scratch, refer to [this notebook](transformer_from_scratch_tutorial/BERT.ipynb).
