{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c38fb8b9",
   "metadata": {},
   "source": [
    "## **BERT** \n",
    ">__[*original Turtorial Link*](https://www.tensorflow.org/text/tutorials/transformer)__\n",
    "\n",
    "### Turtorial Summary\n",
    "1. Downloading the Dataset\n",
    "2. Text tokenization & detokenization\n",
    "3. Setup of Input Pipeline \n",
    "4. Positional Encoding\n",
    "5. Buidling Transformer Model from Scracth\n",
    "6. Setting Hyperparameters \n",
    "7. Training and CheckPointing\n",
    "8. Attention Plot\n",
    "9. Exporting the saved model\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ffeb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d7cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad6b0ba5",
   "metadata": {},
   "source": [
    "#### Downloading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9555412",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading the Dataset\n",
    "## useing Tensorflow datasets\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017a9efe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "mas e se estes fatores fossem ativos ?\n",
      "mas eles não tinham a curiosidade de me testar .\n",
      "\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "##if got time look more for what does the take() method refers to \n",
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "    for pt in pt_examples.numpy():\n",
    "        print(pt.decode('utf-8'))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for en in en_examples.numpy():\n",
    "        print(en.decode('utf-8'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6dce6c3",
   "metadata": {},
   "source": [
    "### tokenization & detokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94c266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to train a NLP model first we need to tokenized them \n",
    "##Converting the text to sequence of token IDS \n",
    "## In simple words converting a sentence or words to list of unique tonek-IDS\n",
    "## for tekenization, in this turtorial we will be using frozen model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc11ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\ted_hrlr_translate_pt_en_converter.zip'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
    "tf.keras.utils.get_file(\n",
    "    f'{model_name}.zip',\n",
    "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c10c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading the model\n",
    "tokenizers=tf.saved_model.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b40e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n",
       "       b'but what if it were active ?',\n",
       "       b\"but they did n't test for curiosity .\"], dtype=object)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56254ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Example of tokenization \n",
    "encoded_en_examples=tokenizers.en.tokenize(en_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a330803",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n",
      "[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n",
      "[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n"
     ]
    }
   ],
   "source": [
    "## encoded tokens of above en_examples\n",
    "for row in encoded_en_examples.to_list():\n",
    "    print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43146852",
   "metadata": {},
   "source": [
    "*the shape of each input array is not same as the lenght of the sentence is not fixed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23c00f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n ' t test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "### Decoding the encoded english examples \n",
    "round_trip = tokenizers.en.detokenize(encoded_en_examples)\n",
    "for line in round_trip.numpy():\n",
    "    print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94cba068",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How does the sequence of words are tokenized \n",
    "## using lookup() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa9492c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',\n",
      "  b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',\n",
      "  b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',\n",
      "  b'##ity', b'.', b'[END]']                                                 ,\n",
      " [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',\n",
      "  b'[END]']                                                           ,\n",
      " [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for',\n",
      "  b'curiosity', b'.', b'[END]']                                          ]>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizers.en.lookup(encoded_en_examples)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "076a03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................."
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for pt_examples, en_examples in train_examples.batch(1024):\n",
    "    pt_tokens = tokenizers.en.tokenize(pt_examples)\n",
    "    lengths.append(pt_tokens.row_lengths())\n",
    "    #print(pt_tokens.row_lengths())\n",
    "    en_tokens = tokenizers.en.tokenize(en_examples)\n",
    "    lengths.append(en_tokens.row_lengths())\n",
    "    print('.', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9144f127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1024,), dtype=int64, numpy=array([45, 21, 23, ..., 21, 47, 35], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27,  9, 12, ...,  9, 23, 28], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 32,  22,  34, ...,  39, 101,  36], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([17, 12, 20, ..., 30, 56, 13], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([48, 37, 27, ..., 92, 21, 49], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([29, 18, 15, ..., 53, 13, 29], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([72, 38, 35, ..., 20, 54, 13], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([38, 18, 16, ..., 11, 26, 11], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 15,  24,  51, ...,  27,  40, 117], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 16, 24, ..., 20, 24, 71], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([28, 44, 26, ..., 95, 34, 32], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([17, 28, 14, ..., 39, 23, 22], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([35, 28, 43, ..., 71, 20, 18], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 15, 26, ..., 34,  7, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 32,  60,  25, ...,  37, 101,  31], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([18, 29, 15, ..., 22, 42, 21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([16, 64, 26, ..., 38, 65, 41], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11, 28, 12, ..., 15, 25, 24], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([21, 74, 30, ..., 32, 84, 71], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([12, 40, 10, ..., 20, 30, 34], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([56, 54, 38, ..., 34, 22, 23], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([45, 22, 21, ..., 14, 10, 16], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 30,  25,  56, ..., 114,  34,  75], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([16, 14, 30, ..., 47, 17, 34], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([26, 24, 28, ..., 17, 29, 30], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([14, 10, 13, ..., 10, 13, 13], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19, 25, 65, ..., 39, 56, 75], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 14, 33, ..., 15, 26, 39], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([48, 25, 23, ..., 65, 15,  8], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([24,  9, 10, ..., 37, 10,  5], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([50, 35, 56, ..., 33, 23, 58], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([30, 19, 20, ..., 15, 11, 23], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 52, 11, ..., 43, 25, 37], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 34, 10, ..., 25, 11, 21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([52, 83, 31, ..., 33, 98, 31], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([30, 38, 19, ..., 15, 34, 16], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 30,  14,  24, ..., 103,  17,  55], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([17,  8, 34, ..., 51,  9, 37], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([36,  9,  8, ..., 55, 31, 37], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19, 10,  5, ..., 28, 13, 17], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 39,  34,  19, ...,  43, 115,  26], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19, 19, 10, ..., 29, 65,  9], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([51, 28, 57, ..., 48, 25, 41], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([35, 16, 27, ..., 24, 12, 18], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([52, 32, 80, ..., 14, 62, 44], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27, 19, 42, ..., 11, 31, 20], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([34, 17, 49, ..., 12, 21, 25], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([16, 12, 30, ...,  8, 11, 11], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 68, 119,  19, ...,  11,  21,  35], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([31, 54, 11, ..., 12, 12, 25], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([38, 25, 14, ..., 54, 21, 40], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([25, 10,  6, ..., 24, 10, 27], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 57,  41,  14, ...,  26,  92, 198], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 36,  20,  12, ...,  12,  40, 103], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([25, 30, 40, ..., 35, 36, 30], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 22, 21, ..., 22, 17, 13], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([63, 26, 23, ..., 98, 32, 42], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([35, 16, 12, ..., 52, 14, 23], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 65, 23, ..., 63, 47, 21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 9, 34, 15, ..., 40, 24, 15], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([12, 21, 37, ..., 28, 16, 25], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10,  8, 16, ..., 11,  9, 12], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([99, 82, 41, ..., 33, 59, 15], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([41, 46, 22, ..., 13, 25,  8], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 15,  25,  64, ..., 158,  25, 103], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([10, 14, 44, ..., 78, 13, 40], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([28, 13, 78, ..., 25, 39, 12], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([19,  9, 31, ..., 12, 22, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([20,  9, 90, ..., 21, 66, 17], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11,  5, 34, ..., 12, 27, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([127,  28,  35, ...,  75,  43,  14], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([87, 16, 12, ..., 34, 18, 13], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([  8,  27,  90, ...,  48,  10, 227], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([  5,  19,  38, ...,  23,   4, 130], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([65, 69, 37, ..., 13,  9, 11], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([32, 36, 19, ..., 13,  5, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27, 37, 21, ..., 49, 15, 23], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([20, 17, 11, ..., 28, 13, 15], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11, 24, 75, ..., 40,  9, 15], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 8, 12, 53, ..., 21,  8, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([21, 55, 24, ..., 36, 34, 36], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([15, 27, 10, ..., 24, 19, 23], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([46, 38, 28, ..., 56, 53, 40], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([27, 27, 11, ..., 26, 24, 21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 42, 241,  50, ...,  64,  40,  54], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 15, 117,  23, ...,  31,  24,  21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([63, 29, 29, ..., 49, 24, 63], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([24,  8, 19, ..., 33, 13, 32], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([43, 27, 28, ..., 76, 51, 46], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([33, 13, 11, ..., 43, 24, 16], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([63, 28, 21, ..., 21, 46, 45], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([24, 16, 12, ..., 10, 23, 26], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([76, 26, 52, ..., 29, 53, 15], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([40, 19, 23, ..., 19, 20, 10], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([106,  15,  14, ...,  73,  25,  56], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([44,  9,  9, ..., 32, 18, 32], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([44, 30, 33, ..., 52, 42, 38], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([18, 13, 21, ..., 27, 16, 21], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([11, 31, 12, ..., 98, 19, 24], dtype=int64)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 7, 19, 11, ..., 73,  9, 14], dtype=int64)>,\n",
       " <tf.Tensor: shape=(585,), dtype=int64, numpy=\n",
       " array([ 25,  22,  11,  26,  34,  34,   8,  63,  15,  37,  71,  67,  18,\n",
       "         11,  53, 109,  84, 143,  20,  17,  30,  30,  19,  10,  46,  30,\n",
       "         24,  23,  22,  60, 153,  19,  69,  20,  18,  20,  40, 113,  54,\n",
       "          9,  20,  15,  43,  48,  55,  34,  55,  38,  56,  14,  24,  20,\n",
       "         45,  28,  32,  26,  18,  15,  40,  57,  32,  28,  39,  13,  30,\n",
       "         89,  18,  48,  57,  40, 128,  25,  36,  70,  94,  71,  85,  29,\n",
       "        148,  48,  39,  50,  33, 111,  19,  31,  19,  45,  34,  29, 100,\n",
       "         30,  28,  25,  66,  76,  36,  67,  18,  55,  50,  35,  85,  17,\n",
       "         51, 130,  19,  14,  33,  23,  84, 108,  34,  24,   9,  23,  18,\n",
       "         17,  32,  67,  21, 135,  24,  53,  57,  78,  43,  30,  36,  22,\n",
       "         54,  24,  34,  62,  65,  63,  10,  87, 151,  16,  53,  41,   9,\n",
       "         45, 138,  29,  67, 122,  31,  33,  27,  53,  17, 138,  54,  81,\n",
       "         33,  35,  81,  32,  46,  29,  25,  13,  55,  92,   9,  29,  48,\n",
       "         54,  25,  75,  91, 109,  56,  63,  54,   9,  65,  15,  39,  39,\n",
       "         23,  23,  78,  21,  60,  66,  65,  30,  22,  27,  51,  29,  31,\n",
       "         27,  42,  49,  31,  19,  13,  71,  25,  23,  81,  12,  88,  16,\n",
       "         99,  18,  31,  20, 106,  21,  11,  29,  71,  21,  47,  20,  19,\n",
       "         72,  14,  34,  58,  18,   9,  15, 248,  67,  50,  94,  22, 150,\n",
       "         49,  14,  37,  80,  47,  89,  29,  60,  30,  98,  24,  91,   9,\n",
       "         44,  26, 131,  43,  50,  10,  40,  19,  87,  58,  20,  17,  27,\n",
       "          8,  35,  15,  67,  33,  28,  11,  39,  27,  18,  39,  45,  37,\n",
       "         86,  27, 106,  36,  26,  20,  53,  31,  41,  24,  24,  21,  51,\n",
       "         18,  55,  10,  29, 178,  65,  33,  24,  99,  44,  91,  18,  24,\n",
       "         41,  28,  18,  37,  83,  23,  73,  21,  27,  35,  87,   7,  33,\n",
       "         79,  28,  29,  42,  75,  31,  28,  26,  17, 211,  19,  31,  94,\n",
       "         36,  26,  43,  24,  50,  40,  20,  42,  39, 169,   8,  53,  18,\n",
       "         37,  84,  26,  90,  78,  18,  53,  25,  28,  40,  16,  36,  24,\n",
       "         44,  32,  22,  59, 140,  57,  62,  29,  49,  48,  45,  20,  45,\n",
       "         47,  47,  57, 136,  29,  18,  62,  70,  14,  45,  38,  49,  74,\n",
       "         37,  23,  16,  34,  38,  19,  18,  16,  88,  32,  78,  49,  88,\n",
       "         26,  44,  16,  31,  30,  52, 137, 114,  33,  26,  22,  24, 172,\n",
       "         32,  28,  90,  15,  27,  91, 108,  52,   9,  11,  45,  24,  37,\n",
       "         99,  19,  24, 107,  48,  70,  22,  85, 111, 178,  30,  38, 118,\n",
       "         85,  10,  33,   8,  22,  29,  19,  28,  21, 177,  24,  55,  30,\n",
       "         17, 135,  27,  22,  31,  34,  20,  33,  34, 142,  23,  75,  45,\n",
       "         32,  21,  21,  15,  31,  32,  30,  10, 158, 148,   9,  45,  19,\n",
       "         55,  24,  47,  66,  16,  86,  55,  48,  53,  19,  21,  39,  22,\n",
       "         15,  20,  84,  29,  38,  77,  28,  79, 187,  57,  23,  61,  23,\n",
       "         74,  41,  55,  32,  79,  32,  30,  54,   7,  50,  16,  21,  25,\n",
       "         51,  31,  14,  16,  51,  35,  36,  16,  50,  56,  59,  47,  34,\n",
       "        140,  37,  38,  20,  56,  46,  65,  58,   8,  62,  10,  71,  17,\n",
       "         25,  28, 119,  48,  20, 104,  20,   7,  19, 140,  25,  61, 128,\n",
       "         94,  22,  49,  86,  45,  19,  80,  41,  32,  81,  92,  26,  29,\n",
       "         57,  18,  22,  16,  16,  28,  18,  83,  55,  15,  35,  18,  42,\n",
       "         25,  30,  53,  32,  69,  23,  27,  25,  45,  57, 102,  16,  46],\n",
       "       dtype=int64)>,\n",
       " <tf.Tensor: shape=(585,), dtype=int64, numpy=\n",
       " array([ 16,  12,  10,  17,  18,  15,   5,  27,   8,  18,  38,  29,  11,\n",
       "          6,  30,  54,  46,  75,  12,  10,  16,  13,  11,   5,  28,  13,\n",
       "         14,  14,  12,  32,  63,   7,  39,   8,  15,  18,  27,  54,  26,\n",
       "          5,   8,  10,  25,  21,  29,  16,  32,  17,  29,   8,  13,   9,\n",
       "         26,  13,  21,  17,  14,   7,  23,  31,  16,  16,  20,  14,  17,\n",
       "         51,  10,  27,  25,  15,  73,  16,  15,  34,  49,  33,  46,  19,\n",
       "         56,  26,  24,  21,  14,  41,  14,  19,   8,  22,  19,  14,  35,\n",
       "         15,  16,  17,  30,  41,  16,  36,  13,  28,  36,  16,  58,   8,\n",
       "         24,  64,  12,  11,  14,  10,  59,  51,  21,  15,   6,  13,  10,\n",
       "         10,  15,  29,  15,  86,  15,  27,  33,  43,  22,  18,  16,  10,\n",
       "         36,  12,  19,  48,  31,  27,   6,  46,  71,  11,  24,  24,   5,\n",
       "         22,  73,  19,  29,  58,  18,  17,  18,  30,  11,  59,  29,  42,\n",
       "         19,  19,  40,  15,  28,  21,   8,  13,  34,  48,   6,  18,  23,\n",
       "         27,  10,  34,  44,  53,  28,  32,  46,   5,  23,   9,  15,  25,\n",
       "         18,  12,  50,  14,  38,  29,  38,  15,  17,   9,  30,  16,  24,\n",
       "         13,  24,  26,  14,  10,  11,  40,  16,  17,  43,   8,  48,   6,\n",
       "         59,  13,  19,  15,  45,  10,   7,  21,  37,  11,  34,  16,  14,\n",
       "         41,   8,  13,  30,   9,   5,  11, 134,  30,  27,  42,  17,  85,\n",
       "         28,   7,  26,  38,  28,  41,  15,  32,  12,  59,  16,  46,   5,\n",
       "         22,  14,  65,  18,  25,   7,  17,  12,  54,  25,  12,  14,  15,\n",
       "          5,  18,   8,  27,  17,  18,   9,  14,  15,   9,  20,  35,  17,\n",
       "         45,  13,  37,  21,  14,  24,  28,  16,  22,  12,  19,   9,  33,\n",
       "         13,  29,   8,  19,  83,  28,  15,  18,  54,  25,  41,   8,  13,\n",
       "         29,  16,  15,  17,  61,  13,  28,   9,  15,  18,  35,   6,  16,\n",
       "         43,  16,  20,  24,  43,  16,  19,  16,   9, 123,  11,  17,  43,\n",
       "         17,  33,  22,  17,  29,  11,  13,  27,  22,  90,   5,  31,   7,\n",
       "         20,  47,  10,  62,  34,   9,  28,  16,  17,  16,  21,  15,  15,\n",
       "         22,  17,  13,  29,  60,  23,  28,  12,  29,  19,  22,  15,  25,\n",
       "         26,  30,  29,  58,  22,  11,  35,  41,  12,  25,  21,  24,  33,\n",
       "         27,  12,  17,  16,  20,   9,  10,  10,  46,  18,  42,  21,  49,\n",
       "         13,  23,  12,  16,  20,  21,  64,  50,  19,  17,   9,  11,  94,\n",
       "         16,  16,  41,  12,  13,  51,  75,  19,   8,   8,  30,  15,  23,\n",
       "         45,  11,  16,  40,  22,  33,  10,  58,  53,  88,  15,  27,  59,\n",
       "         44,   6,  16,   5,  13,  11,  16,  20,  13,  73,  14,  21,  12,\n",
       "         12,  61,  15,  13,  15,  19,  12,  20,  13,  89,  10,  39,  26,\n",
       "         16,  11,   8,  10,  19,  20,  19,   5,  88,  65,   5,  20,   7,\n",
       "         32,  15,  19,  32,  11,  43,  27,  20,  32,  11,  14,  24,  13,\n",
       "         10,  12,  31,  15,  18,  42,  13,  50,  75,  37,  12,  31,  11,\n",
       "         29,  18,  33,  19,  32,  14,  13,  27,   7,  25,  11,  11,  11,\n",
       "         18,  16,   7,   9,  16,  14,  20,   8,  26,  26,  23,  18,  20,\n",
       "         72,  16,  15,  14,  18,  21,  36,  35,   4,  24,   7,  25,  11,\n",
       "         15,  19,  50,  33,  13,  55,  12,   4,  12,  75,  13,  28,  75,\n",
       "         47,  14,  19,  30,  21,  13,  29,  22,  18,  36,  39,  15,  16,\n",
       "         27,  10,  11,   8,  10,  21,  15,  34,  28,   7,  16,   9,  17,\n",
       "         11,  16,  41,  16,  31,  12,  15,  12,  26,  30,  49,  16,  18],\n",
       "       dtype=int64)>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd13a516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_lengths = np.concatenate(lengths)\n",
    "\n",
    "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
    "plt.ylim(plt.ylim())\n",
    "max_length = max(all_lengths)\n",
    "plt.plot([max_length, max_length], plt.ylim())\n",
    "plt.title(f'Max tokens per example: {max_length}');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa415c85",
   "metadata": {},
   "source": [
    "### Setting Input Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aaaa32af",
   "metadata": {},
   "source": [
    "*Defining max_tokens*<br>\n",
    ">max_tokens: Here max num of words for each batch size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33a948f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76c13dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pairs(pt, en):\n",
    "    #here pt and en are pt_examples[i] and en_examples[i]\n",
    "    pt = tokenizers.pt.tokenize(pt)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    pt = pt.to_tensor()\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    en = en.to_tensor()\n",
    "    return pt, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dafc158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading the Dataset\n",
    "## useing Tensorflow datasets\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02c4f892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt\n",
      " [[  2  88 120 ...   0   0   0]\n",
      " [  2 175 153 ...   0   0   0]\n",
      " [  2 101 105 ...   0   0   0]\n",
      " ...\n",
      " [  2 103 770 ...   0   0   0]\n",
      " [  2 133  14 ...   0   0   0]\n",
      " [  2  91 301 ...   0   0   0]] \n",
      "\n",
      " en\n",
      "\n",
      " [[  2 110  13 ...   0   0   0]\n",
      " [  2  45 628 ...   0   0   0]\n",
      " [  2  87 140 ...   0   0   0]\n",
      " ...\n",
      " [  2  77  71 ...   0   0   0]\n",
      " [  2 110  13 ...   0   0   0]\n",
      " [  2  99 474 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "## example\n",
    "##pt_example, and en_example are from row number \"Downloading Dataset 51 number\"\n",
    "pt,en=tokenize_pairs(pt_examples,en_examples)\n",
    "print('pt\\n {} \\n\\n en\\n\\n {}'.format(pt,en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7faad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt.shape: (585, 147)\n",
      "en.shape: (585, 134)\n"
     ]
    }
   ],
   "source": [
    "print('pt.shape: {}\\nen.shape: {}'.format(pt.shape,en.shape))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04e5f81b",
   "metadata": {},
   "source": [
    "*bool function to filter out inputs with num_tokens*\n",
    "<br>tokenize pair are feeded to this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88e6a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_tokens(pt, en):\n",
    "    ##Here pt and en are tokenize value of pt_examples and en_examples\n",
    "    num_tokens = tf.maximum(tf.shape(pt)[1],tf.shape(en)[1])\n",
    "    return num_tokens < MAX_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14661fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(False, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "### Example of above function \n",
    "bool_tensor=filter_max_tokens(pt,en)\n",
    "print(bool_tensor)\n",
    "## meaning num_tokens is not less than the max tokens "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88bbaf5e",
   "metadata": {},
   "source": [
    "**Example of tf.maximum**\n",
    "<code>\n",
    "    x = tf.constant([-5., -1., 0., 0.])\n",
    "    y = tf.constant([-3.])\n",
    "    c=tf.math.maximum(x, y)\n",
    "    print(c)\n",
    "</code>    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "270ee05b",
   "metadata": {},
   "source": [
    "Here's a simple input pipeline that processes, shuffles and batches the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5ad4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca7d77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "    return (\n",
    "      ds\n",
    "      .cache()\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)## Define above\n",
    "      .filter(filter_max_tokens) #Define above\n",
    "      .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6ad489e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 93)\n",
      "(64, 100)\n",
      "(64, 92)\n",
      "(64, 69)\n",
      "(64, 56)\n"
     ]
    }
   ],
   "source": [
    "## Here we can observe that each batch size still have variable length\n",
    "\n",
    "\n",
    "## Question: During the training process how do they handle different sequence length?\n",
    "for element in train_batches.take(5):\n",
    "    print(element[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dbcc216",
   "metadata": {},
   "source": [
    "### Positional Encoding \n",
    "\n",
    ">>look into *An Attention is all we need* paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dddd48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60a5d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f36c66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20, 36), dtype=float32, numpy=\n",
       "array([[[ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "        [ 8.41470957e-01,  5.40302277e-01,  5.64216733e-01,\n",
       "          8.25626731e-01,  3.51695180e-01,  9.36114550e-01,\n",
       "          2.13780671e-01,  9.76881683e-01,  1.28796190e-01,\n",
       "          9.91671085e-01,  7.73490295e-02,  9.97004092e-01,\n",
       "          4.63992245e-02,  9.98923004e-01,  2.78220028e-02,\n",
       "          9.99612868e-01,  1.66802313e-02,  9.99860883e-01,\n",
       "          9.99983307e-03,  9.99949992e-01,  5.99480653e-03,\n",
       "          9.99982059e-01,  3.59380594e-03,  9.99993563e-01,\n",
       "          2.15443294e-03,  9.99997675e-01,  1.29154930e-03,\n",
       "          9.99999166e-01,  7.74263579e-04,  9.99999702e-01,\n",
       "          4.64158860e-04,  9.99999881e-01,  2.78255931e-04,\n",
       "          9.99999940e-01,  1.66810059e-04,  1.00000000e+00],\n",
       "        [ 9.09297407e-01, -4.16146845e-01,  9.31664824e-01,\n",
       "          3.63318950e-01,  6.58454001e-01,  7.52620995e-01,\n",
       "          4.17676836e-01,  9.08595681e-01,  2.55446911e-01,\n",
       "          9.66823101e-01,  1.54234603e-01,  9.88034248e-01,\n",
       "          9.26984996e-02,  9.95694220e-01,  5.56224659e-02,\n",
       "          9.98451889e-01,  3.33558209e-02,  9.99443531e-01,\n",
       "          1.99986659e-02,  9.99800026e-01,  1.19893979e-02,\n",
       "          9.99928117e-01,  7.18756532e-03,  9.99974191e-01,\n",
       "          4.30885609e-03,  9.99990702e-01,  2.58309650e-03,\n",
       "          9.99996662e-01,  1.54852669e-03,  9.99998808e-01,\n",
       "          9.28317662e-04,  9.99999583e-01,  5.56511863e-04,\n",
       "          9.99999821e-01,  3.33620090e-04,  9.99999940e-01],\n",
       "        [ 1.41120002e-01, -9.89992499e-01,  9.74197984e-01,\n",
       "         -2.25695044e-01,  8.81081522e-01,  4.72964376e-01,\n",
       "          6.02261007e-01,  7.98299193e-01,  3.77842456e-01,\n",
       "          9.25869882e-01,  2.30196014e-01,  9.73144293e-01,\n",
       "          1.38798103e-01,  9.90320683e-01,  8.33798647e-02,\n",
       "          9.96517837e-01,  5.00221327e-02,  9.98748124e-01,\n",
       "          2.99955010e-02,  9.99550045e-01,  1.79835577e-02,\n",
       "          9.99838293e-01,  1.07812323e-02,  9.99941885e-01,\n",
       "          6.46325899e-03,  9.99979138e-01,  3.87463928e-03,\n",
       "          9.99992490e-01,  2.32278905e-03,  9.99997318e-01,\n",
       "          1.39247614e-03,  9.99999046e-01,  8.34767707e-04,\n",
       "          9.99999642e-01,  5.00430120e-04,  9.99999881e-01],\n",
       "        [-7.56802499e-01, -6.53643608e-01,  6.76982999e-01,\n",
       "         -7.35998690e-01,  9.91132557e-01,  1.32876709e-01,\n",
       "          7.58998692e-01,  6.51092112e-01,  4.93943959e-01,\n",
       "          8.69493723e-01,  3.04778129e-01,  9.52423394e-01,\n",
       "          1.84598729e-01,  9.82813954e-01,  1.11072712e-01,\n",
       "          9.93812263e-01,  6.66745231e-02,  9.97774780e-01,\n",
       "          3.99893336e-02,  9.99200106e-01,  2.39770729e-02,\n",
       "          9.99712527e-01,  1.43747600e-02,  9.99896705e-01,\n",
       "          8.61763209e-03,  9.99962866e-01,  5.16617578e-03,\n",
       "          9.99986649e-01,  3.09704989e-03,  9.99995232e-01,\n",
       "          1.85663451e-03,  9.99998271e-01,  1.11302349e-03,\n",
       "          9.99999404e-01,  6.67240180e-04,  9.99999762e-01],\n",
       "        [-9.58924294e-01,  2.83662200e-01,  1.43672481e-01,\n",
       "         -9.89625275e-01,  9.74545717e-01, -2.24188745e-01,\n",
       "          8.80642831e-01,  4.73780721e-01,  6.01817429e-01,\n",
       "          7.98633695e-01,  3.77534062e-01,  9.25995708e-01,\n",
       "          2.30001718e-01,  9.73190248e-01,  1.38679564e-01,\n",
       "          9.90337312e-01,  8.33083615e-02,  9.96523798e-01,\n",
       "          4.99791689e-02,  9.98750269e-01,  2.99697239e-02,\n",
       "          9.99550819e-01,  1.79681014e-02,  9.99838531e-01,\n",
       "          1.07719647e-02,  9.99942005e-01,  6.45770365e-03,\n",
       "          9.99979138e-01,  3.87130864e-03,  9.99992490e-01,\n",
       "          2.32079229e-03,  9.99997318e-01,  1.39127928e-03,\n",
       "          9.99999046e-01,  8.34050181e-04,  9.99999642e-01],\n",
       "        [-2.79415488e-01,  9.60170269e-01, -4.39743310e-01,\n",
       "         -8.98123503e-01,  8.33440363e-01, -5.52609384e-01,\n",
       "          9.61569011e-01,  2.74563283e-01,  6.99665904e-01,\n",
       "          7.14470148e-01,  4.48027879e-01,  8.94019604e-01,\n",
       "          2.74909258e-01,  9.61470187e-01,  1.66179046e-01,\n",
       "          9.86095607e-01,  9.99190211e-02,  9.94995594e-01,\n",
       "          5.99640049e-02,  9.98200536e-01,  3.59613001e-02,\n",
       "          9.99353170e-01,  2.15612110e-02,  9.99767542e-01,\n",
       "          1.29262479e-02,  9.99916434e-01,  7.74922036e-03,\n",
       "          9.99969959e-01,  4.64556552e-03,  9.99989212e-01,\n",
       "          2.78494973e-03,  9.99996126e-01,  1.66953483e-03,\n",
       "          9.99998629e-01,  1.00086012e-03,  9.99999523e-01],\n",
       "        [ 6.56986594e-01,  7.53902256e-01, -8.69800150e-01,\n",
       "         -4.93404210e-01,  5.85845590e-01, -8.10422659e-01,\n",
       "          9.98035491e-01,  6.26509860e-02,  7.85859525e-01,\n",
       "          6.18405104e-01,  5.15837193e-01,  8.56686652e-01,\n",
       "          3.19224656e-01,  9.47679043e-01,  1.93549871e-01,\n",
       "          9.81090426e-01,  1.16501875e-01,  9.93190467e-01,\n",
       "          6.99428469e-02,  9.97551024e-01,  4.19515818e-02,\n",
       "          9.99119639e-01,  2.51540430e-02,  9.99683559e-01,\n",
       "          1.50804715e-02,  9.99886274e-01,  9.04072449e-03,\n",
       "          9.99959111e-01,  5.41981915e-03,  9.99985337e-01,\n",
       "          3.24910646e-03,  9.99994695e-01,  1.94779038e-03,\n",
       "          9.99998093e-01,  1.16767013e-03,  9.99999344e-01],\n",
       "        [ 9.89358246e-01, -1.45500034e-01, -9.96517122e-01,\n",
       "          8.33880752e-02,  2.63396859e-01, -9.64687586e-01,\n",
       "          9.88356173e-01, -1.52158096e-01,  8.58962357e-01,\n",
       "          5.12038708e-01,  5.80555618e-01,  8.14220548e-01,\n",
       "          3.62852424e-01,  9.31846619e-01,  2.20770851e-01,\n",
       "          9.75325704e-01,  1.33052319e-01,  9.91109014e-01,\n",
       "          7.99146965e-02,  9.96801734e-01,  4.79403585e-02,\n",
       "          9.98850226e-01,  2.87465490e-02,  9.99586761e-01,\n",
       "          1.72346234e-02,  9.99851465e-01,  1.03322137e-02,\n",
       "          9.99946594e-01,  6.19406998e-03,  9.99980807e-01,\n",
       "          3.71326250e-03,  9.99993086e-01,  2.22604559e-03,\n",
       "          9.99997497e-01,  1.33448001e-03,  9.99999106e-01],\n",
       "        [ 4.12118495e-01, -9.11130250e-01, -7.75702238e-01,\n",
       "          6.31099045e-01, -9.27063376e-02, -9.95693505e-01,\n",
       "          9.32978570e-01, -3.59931886e-01,  9.17756796e-01,\n",
       "          3.97142917e-01,  6.41795516e-01,  7.66875803e-01,\n",
       "          4.05698568e-01,  9.14006948e-01,  2.47820899e-01,\n",
       "          9.68805850e-01,  1.49565727e-01,  9.88751769e-01,\n",
       "          8.98785517e-02,  9.95952725e-01,  5.39274104e-02,\n",
       "          9.98544872e-01,  3.23386826e-02,  9.99476969e-01,\n",
       "          1.93886980e-02,  9.99812007e-01,  1.16236852e-02,\n",
       "          9.99932468e-01,  6.96831662e-03,  9.99975741e-01,\n",
       "          4.17741761e-03,  9.99991298e-01,  2.50430079e-03,\n",
       "          9.99996841e-01,  1.50128989e-03,  9.99998868e-01],\n",
       "        [-5.44021130e-01, -8.39071512e-01, -2.84363836e-01,\n",
       "          9.58716452e-01, -4.36964363e-01, -8.99478793e-01,\n",
       "          8.34463179e-01, -5.51063657e-01,  9.61263359e-01,\n",
       "          2.75631577e-01,  6.99189842e-01,  7.14936078e-01,\n",
       "          4.47670847e-01,  8.94198418e-01,  2.74679095e-01,\n",
       "          9.61535931e-01,  1.66037530e-01,  9.86119449e-01,\n",
       "          9.98334140e-02,  9.95004177e-01,  5.99125251e-02,\n",
       "          9.98203635e-01,  3.59304026e-02,  9.99354303e-01,\n",
       "          2.15426795e-02,  9.99767959e-01,  1.29151372e-02,\n",
       "          9.99916613e-01,  7.74255954e-03,  9.99970019e-01,\n",
       "          4.64157201e-03,  9.99989212e-01,  2.78255576e-03,\n",
       "          9.99996126e-01,  1.66809978e-03,  9.99998629e-01],\n",
       "        [-9.99990225e-01,  4.42569796e-03,  3.06145489e-01,\n",
       "          9.51984763e-01, -7.25391090e-01, -6.88336968e-01,\n",
       "          6.97365046e-01, -7.16716111e-01,  9.88757372e-01,\n",
       "          1.49528801e-01,  7.52394736e-01,  6.58712506e-01,\n",
       "          4.88678783e-01,  8.72463763e-01,  3.01324606e-01,\n",
       "          9.53521609e-01,  1.82463139e-01,  9.83212709e-01,\n",
       "          1.09778300e-01,  9.93956089e-01,  6.58954829e-02,\n",
       "          9.97826517e-01,  3.95216532e-02,  9.99218702e-01,\n",
       "          2.36965641e-02,  9.99719203e-01,  1.42065687e-02,\n",
       "          9.99899089e-01,  8.51679780e-03,  9.99963760e-01,\n",
       "          5.10572549e-03,  9.99986947e-01,  3.06081050e-03,\n",
       "          9.99995291e-01,  1.83490955e-03,  9.99998331e-01],\n",
       "        [-5.36572933e-01,  8.43853951e-01,  7.89887607e-01,\n",
       "          6.13251626e-01, -9.21133935e-01, -3.89245719e-01,\n",
       "          5.28023124e-01, -8.49229991e-01,  9.99780834e-01,\n",
       "          2.09351983e-02,  8.01091373e-01,  5.98542035e-01,\n",
       "          5.28634131e-01,  8.48849773e-01,  3.27736855e-01,\n",
       "          9.44769025e-01,  1.98837966e-01,  9.80032384e-01,\n",
       "          1.19712204e-01,  9.92808640e-01,  7.18760788e-02,\n",
       "          9.97413576e-01,  4.31123972e-02,  9.99070227e-01,\n",
       "          2.58503370e-02,  9.99665797e-01,  1.54979751e-02,\n",
       "          9.99879897e-01,  9.29103047e-03,  9.99956846e-01,\n",
       "          5.56987757e-03,  9.99984503e-01,  3.33906501e-03,\n",
       "          9.99994397e-01,  2.00171932e-03,  9.99997973e-01],\n",
       "        [ 4.20167029e-01,  9.07446802e-01,  9.98159170e-01,\n",
       "          6.06491379e-02, -9.99182761e-01, -4.04202044e-02,\n",
       "          3.34267169e-01, -9.42478359e-01,  9.94150102e-01,\n",
       "         -1.08007133e-01,  8.44988048e-01,  5.34785211e-01,\n",
       "          5.67450762e-01,  8.23407352e-01,  3.53895366e-01,\n",
       "          9.35285032e-01,  2.15157464e-01,  9.76579368e-01,\n",
       "          1.29634142e-01,  9.91561890e-01,  7.78540894e-02,\n",
       "          9.96964753e-01,  4.67025824e-02,  9.98908818e-01,\n",
       "          2.80039888e-02,  9.99607801e-01,  1.67893562e-02,\n",
       "          9.99859035e-01,  1.00652575e-02,  9.99949336e-01,\n",
       "          6.03402872e-03,  9.99981821e-01,  3.61731928e-03,\n",
       "          9.99993443e-01,  2.16852897e-03,  9.99997675e-01],\n",
       "        [ 9.90607381e-01,  1.36737213e-01,  8.58326137e-01,\n",
       "         -5.13104558e-01, -9.49565172e-01,  3.13569844e-01,\n",
       "          1.25055820e-01, -9.92149711e-01,  9.71958995e-01,\n",
       "         -2.35150307e-01,  8.83821607e-01,  4.67824042e-01,\n",
       "          6.05045021e-01,  7.96191216e-01,  3.79779875e-01,\n",
       "          9.25076902e-01,  2.31417105e-01,  9.72854614e-01,\n",
       "          1.39543116e-01,  9.90216017e-01,  8.38292986e-02,\n",
       "          9.96480107e-01,  5.02921678e-02,  9.98734534e-01,\n",
       "          3.01575121e-02,  9.99545157e-01,  1.80807095e-02,\n",
       "          9.99836504e-01,  1.08394790e-02,  9.99941230e-01,\n",
       "          6.49817847e-03,  9.99978900e-01,  3.89557332e-03,\n",
       "          9.99992430e-01,  2.33533862e-03,  9.99997258e-01],\n",
       "        [ 6.50287867e-01, -7.59687901e-01,  4.19154823e-01,\n",
       "         -9.07914758e-01, -7.78620780e-01,  6.27494752e-01,\n",
       "         -8.99376869e-02, -9.95947421e-01,  9.33577180e-01,\n",
       "         -3.58376384e-01,  9.17359531e-01,  3.98059726e-01,\n",
       "          6.41336024e-01,  7.67260134e-01,  4.05370325e-01,\n",
       "          9.14152563e-01,  2.47612342e-01,  9.68859196e-01,\n",
       "          1.49438128e-01,  9.88771081e-01,  8.98014978e-02,\n",
       "          9.95959699e-01,  5.38811013e-02,  9.98547375e-01,\n",
       "          3.23108956e-02,  9.99477863e-01,  1.93720330e-02,\n",
       "          9.99812365e-01,  1.16136940e-02,  9.99932587e-01,\n",
       "          6.96232682e-03,  9.99975741e-01,  4.17382689e-03,\n",
       "          9.99991298e-01,  2.50214827e-03,  9.99996841e-01],\n",
       "        [-2.87903309e-01, -9.57659483e-01, -1.66195303e-01,\n",
       "         -9.86092865e-01, -5.08191347e-01,  8.61244202e-01,\n",
       "         -3.00772786e-01, -9.53695834e-01,  8.79643977e-01,\n",
       "         -4.75632668e-01,  9.45400715e-01,  3.25910300e-01,\n",
       "          6.76245570e-01,  7.36676276e-01,  4.30646986e-01,\n",
       "          9.02520478e-01,  2.63738692e-01,  9.64594185e-01,\n",
       "          1.59318209e-01,  9.87227261e-01,  9.57704708e-02,\n",
       "          9.95403469e-01,  5.74693382e-02,  9.98347282e-01,\n",
       "          3.44641283e-02,  9.99405921e-01,  2.06633247e-02,\n",
       "          9.99786496e-01,  1.23879025e-02,  9.99923289e-01,\n",
       "          7.42647378e-03,  9.99972403e-01,  4.45208047e-03,\n",
       "          9.99990106e-01,  2.66895769e-03,  9.99996424e-01],\n",
       "        [-9.61397469e-01, -2.75163352e-01, -6.93585396e-01,\n",
       "         -7.20374465e-01, -1.72829896e-01,  9.84951675e-01,\n",
       "         -4.97701138e-01, -8.67348611e-01,  8.11057806e-01,\n",
       "         -5.84965944e-01,  9.67777193e-01,  2.51808077e-01,\n",
       "          7.09698439e-01,  7.04505563e-01,  4.55590189e-01,\n",
       "          8.90189648e-01,  2.79791653e-01,  9.60060716e-01,\n",
       "          1.69182345e-01,  9.85584795e-01,  1.01736002e-01,\n",
       "          9.94811416e-01,  6.10568337e-02,  9.98134315e-01,\n",
       "          3.66172008e-02,  9.99329388e-01,  2.19545811e-02,\n",
       "          9.99758959e-01,  1.31621026e-02,  9.99913394e-01,\n",
       "          7.89061934e-03,  9.99968886e-01,  4.73033357e-03,\n",
       "          9.99988794e-01,  2.83576711e-03,  9.99996006e-01],\n",
       "        [-7.50987232e-01,  6.60316706e-01, -9.79089916e-01,\n",
       "         -2.03427911e-01,  1.84614182e-01,  9.82811093e-01,\n",
       "         -6.71617508e-01, -7.40898073e-01,  7.28961229e-01,\n",
       "         -6.84554994e-01,  9.84354913e-01,  1.76197037e-01,\n",
       "          7.41622627e-01,  6.70817316e-01,  4.80180681e-01,\n",
       "          8.77169609e-01,  2.95766771e-01,  9.55260158e-01,\n",
       "          1.79029569e-01,  9.83843684e-01,  1.07697874e-01,\n",
       "          9.94183660e-01,  6.46435395e-02,  9.97908413e-01,\n",
       "          3.87701057e-02,  9.99248147e-01,  2.32458003e-02,\n",
       "          9.99729753e-01,  1.39362952e-02,  9.99902904e-01,\n",
       "          8.35476257e-03,  9.99965072e-01,  5.00858575e-03,\n",
       "          9.99987483e-01,  3.00257653e-03,  9.99995470e-01],\n",
       "        [ 1.49877205e-01,  9.88704622e-01, -9.23140228e-01,\n",
       "          3.84463400e-01,  5.18469930e-01,  8.55095863e-01,\n",
       "         -8.14480484e-01, -5.80190897e-01,  6.34721696e-01,\n",
       "         -7.72740841e-01,  9.95034516e-01,  9.95302647e-02,\n",
       "          7.71949291e-01,  6.35684133e-01,  5.04399419e-01,\n",
       "          8.63470435e-01,  3.11659575e-01,  9.50193822e-01,\n",
       "          1.88858896e-01,  9.82004225e-01,  1.13655880e-01,\n",
       "          9.93520200e-01,  6.82294145e-02,  9.97669637e-01,\n",
       "          4.09228280e-02,  9.99162316e-01,  2.45369803e-02,\n",
       "          9.99698937e-01,  1.47104794e-02,  9.99891818e-01,\n",
       "          8.81890487e-03,  9.99961138e-01,  5.28683839e-03,\n",
       "          9.99986053e-01,  3.16938572e-03,  9.99994993e-01]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding(20,36)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c6412ce",
   "metadata": {},
   "source": [
    "### Masking\n",
    ">Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.\n",
    "\n",
    "*Note: We don't need masking skeleton based Posed based Action Recognition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d4070f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84b32943",
   "metadata": {},
   "source": [
    "*Similar function in array*\n",
    "<code>\n",
    "    def create_padding_mask(seq):\n",
    "        seq=np.where(seq!=0,0,1)\n",
    "        return seq\n",
    "</code>    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb2bd843",
   "metadata": {},
   "source": [
    "*Masking Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "267dfd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8edd658d",
   "metadata": {},
   "source": [
    "### look ahead mask\n",
    ">The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.<br>This means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6065f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea1f6357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "\n",
    "temp = create_look_ahead_mask(x.shape[1]) \n",
    "#Value with 1 are masked \n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02a921ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f9e4bd8",
   "metadata": {},
   "source": [
    "### Building Transfomrer from scratch\n",
    "i. Multi-Headed Attention Layer <br>\n",
    "1. Linear Layer <br>\n",
    "2. Scaler Dot Product Attention <br>\n",
    "3. Final Linear Layer<br> \n",
    "\n",
    "ii. Encoder <br>\n",
    "iii. Decoder <br>\n",
    "iv. Tranformer Model <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3efe9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    ## shape is determined by simple matrix multiplication rule \n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    #dk is the square root dimension of keys\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f754518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## here d_model is feature dimension as well as dimension of the k\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads): ##(self,*,d_model=dimension of k,num_heads)\n",
    "        \n",
    "        ##  d_model refer to the dimension of the q,v,k\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model) ## Must Define dense layer separately for all because Dense() always expect same shape but q,v,k may have different shape\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "#     Split the last dimension into (num_heads, depth).\n",
    "#     Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "#         #because of this split the computational time is same \n",
    "#         #even when the number of heads is increased\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # q.shape= (batch_size, seq_len, d_model) ;Change the last dimesnion to d_model\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model) \n",
    "        ## this must assume the d_model and feature length is same \n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8f88c16",
   "metadata": {},
   "source": [
    "*Testing the Above MultiHeadedAttention()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91a3a824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([63, 43, 512]), TensorShape([63, 8, 43, 43]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((63,43,64))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebacc06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([63, 43, 64])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1abcc4a6",
   "metadata": {},
   "source": [
    "### Point wise feed forward network\n",
    ">Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c989348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8878d5d3",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "Each encoder layer consists of sublayers:\n",
    "1. Multi-head Attention\n",
    "2. Point wise feed forward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6ef7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # att_ouptput.shape=(batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        ##d_model must be equal to feature_dimension otherwise error in addition x + attn_output\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22c6f116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4539e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        ## To embed to d_model\n",
    "        ## test code in below cell\n",
    "        self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)# Positional encoding for sequence length up to MAX TOKENS\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x += self.pos_encoding[:, :seq_len, :] #Slices out the shape not in after embedding\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11d6b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos_emb.shape:  (1, 128, 512)\n",
      "After Embedding: x.shape  (64, 62, 512)\n",
      "After additing positional information: x.shape  (64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "#try the below code to understand more about embedding and positional \n",
    "emb=tf.keras.layers.Embedding(8500, 512)\n",
    "pos=positional_encoding(MAX_TOKENS,512)\n",
    "print('Pos_emb.shape: ',pos.shape)\n",
    "\n",
    "x=tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200) #input\n",
    "seq_len=x.shape[1]\n",
    "x=emb(x)\n",
    "print('After Embedding: x.shape ',x.shape)\n",
    "x+=pos[:,:seq_len,:]\n",
    "print('After additing positional information: x.shape ',x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ea02f4a",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "emb=tf.keras.layers.Embedding(8500, 512)\n",
    "pos=positional_encoding(MAX_TOKENS,512)\n",
    "print('Pos_emb.shape: ',pos.shape)\n",
    "Pos_emb.shape:  (1, 128, 512)\n",
    "</code>    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "066220f3",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "x=tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200) #input\n",
    "x=emb(x)\n",
    "print('After Embedding: x.shape ',x.shape)\n",
    "</code>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0898ed2d",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "x *= tf.math.sqrt(tf.cast(512, tf.float32))\n",
    "print('x.shape: ',x.shape)\n",
    "</code>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90f31b05",
   "metadata": {},
   "source": [
    "<code> #Python\n",
    "pos=pos[:,:62,:] # 62 =tf.shape(x)[1] \n",
    "print('Pos_emb.shape: ',pos.shape)\n",
    "</code>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bdcdd87",
   "metadata": {},
   "source": [
    "<code>\n",
    "print(x.shape)=TensorShape([64, 62, 512])\n",
    "</code>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7f77dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 69, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, input_vocab_size=8500)\n",
    "temp_input = tf.random.uniform((64, 69), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fe2d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos_emb.shape:  (1, 128, 512)\n"
     ]
    }
   ],
   "source": [
    "emb=tf.keras.layers.Embedding(8500, 512)\n",
    "pos=positional_encoding(MAX_TOKENS,512)\n",
    "print('Pos_emb.shape: ',pos.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905ddb1a",
   "metadata": {},
   "source": [
    "## Decoder layer\n",
    "Each decoder layer consists of sublayers: <br>\n",
    "1. Masked multi-head attention (with look ahead mask and padding mask) \n",
    "2. *** Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublayer.  ***\n",
    "3. Point wise feed forward networks.\n",
    "<br> <br>\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis.\n",
    "\n",
    ">As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output.<b> In other words, the decoder predicts the next token by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab3d1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "               look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask) ## v,k,q order  \n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  ## v and k are from encoder \n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b95b0373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30779629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                   rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(MAX_TOKENS, d_model)\n",
    "\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "               look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d7f3e32",
   "metadata": {},
   "source": [
    "### Creating a Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a095b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                   target_vocab_size, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               input_vocab_size=input_vocab_size, rate=rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               target_vocab_size=target_vocab_size, rate=rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp, tar = inputs\n",
    "\n",
    "        padding_mask, look_ahead_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, training, padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
    "        padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return padding_mask, look_ahead_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37af343b",
   "metadata": {},
   "source": [
    "### Setting the HyperParameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "651dcb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_attention_heads = 8\n",
    "dropout_rate = 0.1\n",
    "num_heads=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db6ba603",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_attention_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c71a447b",
   "metadata": {},
   "source": [
    "### Test the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b7130c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 7765 and target_vocab_size: 7010\n"
     ]
    }
   ],
   "source": [
    "input_vocab_size=tokenizers.pt.get_vocab_size().numpy()\n",
    "target_vocab_size=tokenizers.en.get_vocab_size().numpy()\n",
    "print('input_vocab_size: {} and target_vocab_size: {}'.format(input_vocab_size,target_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c37d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.constant([[1,2,3, 4, 0, 0, 0]])\n",
    "target = tf.constant([[1,2,3, 0]])\n",
    "\n",
    "x, attention = transformer((input, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99522e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 7])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ca5c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 7010)\n",
      "(1, 8, 4, 4)\n",
      "(1, 8, 4, 7)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(attention['decoder_layer1_block1'].shape)\n",
    "print(attention['decoder_layer4_block2'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b6b9d1e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b714cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This learning rate is according to the paper \"Attention is all we need!\"\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fedc524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14440408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added  \n"
     ]
    }
   ],
   "source": [
    "### Added for testing\n",
    "print(\"Added  \")\n",
    "## Add this to Asus Now\n",
    "## Added in Asus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e252117",
   "metadata": {},
   "source": [
    "## Loss and metrics\n",
    "\n",
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "faf2b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "048b3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "\n",
    "  \"\"\"\n",
    "  pred will have a shape of Batch_SIZE,NUM_TOKENS(Sequences),TARGET_VOCAB_SIZE\n",
    "  tf.argmax(pred, axis=2); will have a shape of Batch_SIZE,NUM_TOKENS(Sequences); this shape is same as that of real.shape\n",
    "  \"\"\";\n",
    "  accuracies = tf.equal(real, tf.argmax(pred, axis=2)) ## return arr of bool(True if real_tokens == pred_tokens)\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))# Return arr of bool(True if real does not have 0)\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "731190c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1=tf.Variable([1,2,3,4,5,6,0])\n",
    "arr2=tf.Variable([1,16,18,12,5,6,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8aeb17f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=bool, numpy=array([ True, False, False, False,  True,  True,  True])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.equal(arr1,arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fa99859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=bool, numpy=array([ True,  True,  True,  True,  True,  True, False])>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.logical_not(tf.equal(arr1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "330a7c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16,), dtype=int64, numpy=array([6, 1, 6, 9, 0, 9, 9, 4, 1, 4, 0, 6, 5, 4, 3, 2], dtype=int64)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(tf.random.normal(shape=(16,10)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9eda89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41cd7203",
   "metadata": {},
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b60f4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9d7e696",
   "metadata": {},
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every n epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2cd839d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_path = './checkpoints/train'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print('Latest checkpoint restored!!')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00070409",
   "metadata": {},
   "source": [
    "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
    "\n",
    "For example, `sentence = 'SOS A lion in the jungle is sleeping EOS'` becomes:\n",
    "\n",
    "* `tar_inp =  'SOS A lion in the jungle is sleeping'`\n",
    "* `tar_real = 'A lion in the jungle is sleeping EOS'`\n",
    "\n",
    "A transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next.\n",
    "\n",
    "During training this example uses teacher-forcing (like in the [text generation tutorial](https://www.tensorflow.org/text/tutorials/text_generation)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the model predicts each token, *self-attention* allows it to look at the previous tokens in the input sequence to better predict the next token.\n",
    "\n",
    "To prevent the model from peeking at the expected output the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c025033",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29aac34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83eaeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]  # why except the last token in each token? The last token is \"END\" ; This is done because the training is shifter ri\n",
    "  tar_real = tar[:, 1:] # Why except the first token\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer([inp, tar_inp],\n",
    "                                 training = True)\n",
    "    \n",
    "    # We notice that the transformer is feeded with tar_input(containing SOS and excluding EOS). As a result in the predicted word start from first word after sos and end with EOS. \n",
    "    # Thus the target loss_function as ground truth value we exclude the SOS, Because the prediction is also right shifted. \n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e3329d0",
   "metadata": {},
   "source": [
    "Portuguese is used as the input language and English is the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d15d390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.8613 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.8005 Accuracy 0.0048\n",
      "Epoch 1 Batch 100 Loss 8.6927 Accuracy 0.0213\n",
      "Epoch 1 Batch 150 Loss 8.5726 Accuracy 0.0301\n",
      "Epoch 1 Batch 200 Loss 8.4292 Accuracy 0.0344\n",
      "Epoch 1 Batch 250 Loss 8.2576 Accuracy 0.0370\n",
      "Epoch 1 Batch 300 Loss 8.0669 Accuracy 0.0388\n",
      "Epoch 1 Batch 350 Loss 7.8718 Accuracy 0.0418\n",
      "Epoch 1 Batch 400 Loss 7.6853 Accuracy 0.0479\n",
      "Epoch 1 Batch 450 Loss 7.5205 Accuracy 0.0548\n",
      "Epoch 1 Batch 500 Loss 7.3770 Accuracy 0.0631\n",
      "Epoch 1 Batch 550 Loss 7.2456 Accuracy 0.0721\n",
      "Epoch 1 Batch 600 Loss 7.1216 Accuracy 0.0810\n",
      "Epoch 1 Batch 650 Loss 7.0047 Accuracy 0.0893\n",
      "Epoch 1 Batch 700 Loss 6.8974 Accuracy 0.0967\n",
      "Epoch 1 Loss 6.8945 Accuracy 0.0969\n",
      "Time taken for 1 epoch: 102.85 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 5.5182 Accuracy 0.1935\n",
      "Epoch 2 Batch 50 Loss 5.4015 Accuracy 0.1992\n",
      "Epoch 2 Batch 100 Loss 5.3542 Accuracy 0.2026\n",
      "Epoch 2 Batch 150 Loss 5.3123 Accuracy 0.2063\n",
      "Epoch 2 Batch 200 Loss 5.2799 Accuracy 0.2092\n",
      "Epoch 2 Batch 250 Loss 5.2464 Accuracy 0.2130\n",
      "Epoch 2 Batch 300 Loss 5.2118 Accuracy 0.2169\n",
      "Epoch 2 Batch 350 Loss 5.1856 Accuracy 0.2198\n",
      "Epoch 2 Batch 400 Loss 5.1606 Accuracy 0.2225\n",
      "Epoch 2 Batch 450 Loss 5.1381 Accuracy 0.2249\n",
      "Epoch 2 Batch 500 Loss 5.1114 Accuracy 0.2272\n",
      "Epoch 2 Batch 550 Loss 5.0889 Accuracy 0.2293\n",
      "Epoch 2 Batch 600 Loss 5.0660 Accuracy 0.2315\n",
      "Epoch 2 Batch 650 Loss 5.0451 Accuracy 0.2334\n",
      "Epoch 2 Loss 5.0260 Accuracy 0.2352\n",
      "Time taken for 1 epoch: 84.46 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 4.6543 Accuracy 0.2684\n",
      "Epoch 3 Batch 50 Loss 4.7169 Accuracy 0.2625\n",
      "Epoch 3 Batch 100 Loss 4.6847 Accuracy 0.2656\n",
      "Epoch 3 Batch 150 Loss 4.6842 Accuracy 0.2644\n",
      "Epoch 3 Batch 200 Loss 4.6697 Accuracy 0.2655\n",
      "Epoch 3 Batch 250 Loss 4.6573 Accuracy 0.2663\n",
      "Epoch 3 Batch 300 Loss 4.6481 Accuracy 0.2669\n",
      "Epoch 3 Batch 350 Loss 4.6401 Accuracy 0.2677\n",
      "Epoch 3 Batch 400 Loss 4.6258 Accuracy 0.2688\n",
      "Epoch 3 Batch 450 Loss 4.6114 Accuracy 0.2703\n",
      "Epoch 3 Batch 500 Loss 4.5973 Accuracy 0.2718\n",
      "Epoch 3 Batch 550 Loss 4.5821 Accuracy 0.2737\n",
      "Epoch 3 Batch 600 Loss 4.5675 Accuracy 0.2754\n",
      "Epoch 3 Batch 650 Loss 4.5525 Accuracy 0.2768\n",
      "Epoch 3 Batch 700 Loss 4.5384 Accuracy 0.2784\n",
      "Epoch 3 Loss 4.5380 Accuracy 0.2784\n",
      "Time taken for 1 epoch: 84.85 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 4.2700 Accuracy 0.2934\n",
      "Epoch 4 Batch 50 Loss 4.2741 Accuracy 0.3043\n",
      "Epoch 4 Batch 100 Loss 4.2621 Accuracy 0.3083\n",
      "Epoch 4 Batch 150 Loss 4.2429 Accuracy 0.3098\n",
      "Epoch 4 Batch 200 Loss 4.2217 Accuracy 0.3117\n",
      "Epoch 4 Batch 250 Loss 4.2006 Accuracy 0.3145\n",
      "Epoch 4 Batch 300 Loss 4.1847 Accuracy 0.3166\n",
      "Epoch 4 Batch 350 Loss 4.1720 Accuracy 0.3183\n",
      "Epoch 4 Batch 400 Loss 4.1565 Accuracy 0.3203\n",
      "Epoch 4 Batch 450 Loss 4.1429 Accuracy 0.3218\n",
      "Epoch 4 Batch 500 Loss 4.1279 Accuracy 0.3236\n",
      "Epoch 4 Batch 550 Loss 4.1158 Accuracy 0.3254\n",
      "Epoch 4 Batch 600 Loss 4.1011 Accuracy 0.3272\n",
      "Epoch 4 Batch 650 Loss 4.0859 Accuracy 0.3294\n",
      "Epoch 4 Loss 4.0692 Accuracy 0.3315\n",
      "Time taken for 1 epoch: 83.84 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.7377 Accuracy 0.3664\n",
      "Epoch 5 Batch 50 Loss 3.7982 Accuracy 0.3614\n",
      "Epoch 5 Batch 100 Loss 3.7691 Accuracy 0.3653\n",
      "Epoch 5 Batch 150 Loss 3.7489 Accuracy 0.3687\n",
      "Epoch 5 Batch 200 Loss 3.7274 Accuracy 0.3720\n",
      "Epoch 5 Batch 250 Loss 3.7159 Accuracy 0.3737\n",
      "Epoch 5 Batch 300 Loss 3.7060 Accuracy 0.3751\n",
      "Epoch 5 Batch 350 Loss 3.6936 Accuracy 0.3770\n",
      "Epoch 5 Batch 400 Loss 3.6803 Accuracy 0.3791\n",
      "Epoch 5 Batch 450 Loss 3.6690 Accuracy 0.3807\n",
      "Epoch 5 Batch 500 Loss 3.6557 Accuracy 0.3824\n",
      "Epoch 5 Batch 550 Loss 3.6423 Accuracy 0.3843\n",
      "Epoch 5 Batch 600 Loss 3.6318 Accuracy 0.3859\n",
      "Epoch 5 Batch 650 Loss 3.6222 Accuracy 0.3872\n",
      "Epoch 5 Batch 700 Loss 3.6116 Accuracy 0.3887\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train\\ckpt-1\n",
      "Epoch 5 Loss 3.6112 Accuracy 0.3888\n",
      "Time taken for 1 epoch: 85.81 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 3.5653 Accuracy 0.4011\n",
      "Epoch 6 Batch 50 Loss 3.3127 Accuracy 0.4242\n",
      "Epoch 6 Batch 100 Loss 3.3239 Accuracy 0.4219\n",
      "Epoch 6 Batch 150 Loss 3.3211 Accuracy 0.4220\n",
      "Epoch 6 Batch 200 Loss 3.3237 Accuracy 0.4221\n",
      "Epoch 6 Batch 250 Loss 3.3204 Accuracy 0.4224\n",
      "Epoch 6 Batch 300 Loss 3.3173 Accuracy 0.4229\n",
      "Epoch 6 Batch 350 Loss 3.3101 Accuracy 0.4241\n",
      "Epoch 6 Batch 400 Loss 3.3045 Accuracy 0.4248\n",
      "Epoch 6 Batch 450 Loss 3.2967 Accuracy 0.4258\n",
      "Epoch 6 Batch 500 Loss 3.2875 Accuracy 0.4270\n",
      "Epoch 6 Batch 550 Loss 3.2789 Accuracy 0.4282\n",
      "Epoch 6 Batch 600 Loss 3.2696 Accuracy 0.4295\n",
      "Epoch 6 Batch 650 Loss 3.2636 Accuracy 0.4304\n",
      "Epoch 6 Batch 700 Loss 3.2539 Accuracy 0.4317\n",
      "Epoch 6 Loss 3.2539 Accuracy 0.4317\n",
      "Time taken for 1 epoch: 85.79 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 3.1832 Accuracy 0.4231\n",
      "Epoch 7 Batch 50 Loss 3.0384 Accuracy 0.4574\n",
      "Epoch 7 Batch 100 Loss 3.0130 Accuracy 0.4591\n",
      "Epoch 7 Batch 150 Loss 3.0060 Accuracy 0.4598\n",
      "Epoch 7 Batch 200 Loss 3.0071 Accuracy 0.4604\n",
      "Epoch 7 Batch 250 Loss 3.0010 Accuracy 0.4614\n",
      "Epoch 7 Batch 300 Loss 2.9918 Accuracy 0.4626\n",
      "Epoch 7 Batch 350 Loss 2.9790 Accuracy 0.4643\n",
      "Epoch 7 Batch 400 Loss 2.9686 Accuracy 0.4659\n",
      "Epoch 7 Batch 450 Loss 2.9568 Accuracy 0.4676\n",
      "Epoch 7 Batch 500 Loss 2.9476 Accuracy 0.4690\n",
      "Epoch 7 Batch 550 Loss 2.9423 Accuracy 0.4698\n",
      "Epoch 7 Batch 600 Loss 2.9347 Accuracy 0.4709\n",
      "Epoch 7 Batch 650 Loss 2.9261 Accuracy 0.4721\n",
      "Epoch 7 Loss 2.9220 Accuracy 0.4727\n",
      "Time taken for 1 epoch: 85.82 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.5350 Accuracy 0.5240\n",
      "Epoch 8 Batch 50 Loss 2.6918 Accuracy 0.5002\n",
      "Epoch 8 Batch 100 Loss 2.6767 Accuracy 0.5029\n",
      "Epoch 8 Batch 150 Loss 2.6741 Accuracy 0.5045\n",
      "Epoch 8 Batch 200 Loss 2.6747 Accuracy 0.5043\n",
      "Epoch 8 Batch 250 Loss 2.6739 Accuracy 0.5044\n",
      "Epoch 8 Batch 300 Loss 2.6722 Accuracy 0.5047\n",
      "Epoch 8 Batch 350 Loss 2.6701 Accuracy 0.5049\n",
      "Epoch 8 Batch 400 Loss 2.6614 Accuracy 0.5062\n",
      "Epoch 8 Batch 450 Loss 2.6606 Accuracy 0.5062\n",
      "Epoch 8 Batch 500 Loss 2.6562 Accuracy 0.5068\n",
      "Epoch 8 Batch 550 Loss 2.6537 Accuracy 0.5071\n",
      "Epoch 8 Batch 600 Loss 2.6482 Accuracy 0.5081\n",
      "Epoch 8 Batch 650 Loss 2.6436 Accuracy 0.5088\n",
      "Epoch 8 Batch 700 Loss 2.6402 Accuracy 0.5095\n",
      "Epoch 8 Loss 2.6391 Accuracy 0.5096\n",
      "Time taken for 1 epoch: 85.70 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.4510 Accuracy 0.5511\n",
      "Epoch 9 Batch 50 Loss 2.4159 Accuracy 0.5392\n",
      "Epoch 9 Batch 100 Loss 2.4167 Accuracy 0.5397\n",
      "Epoch 9 Batch 150 Loss 2.4237 Accuracy 0.5376\n",
      "Epoch 9 Batch 200 Loss 2.4253 Accuracy 0.5372\n",
      "Epoch 9 Batch 250 Loss 2.4259 Accuracy 0.5368\n",
      "Epoch 9 Batch 300 Loss 2.4316 Accuracy 0.5360\n",
      "Epoch 9 Batch 350 Loss 2.4332 Accuracy 0.5358\n",
      "Epoch 9 Batch 400 Loss 2.4345 Accuracy 0.5356\n",
      "Epoch 9 Batch 450 Loss 2.4322 Accuracy 0.5358\n",
      "Epoch 9 Batch 500 Loss 2.4298 Accuracy 0.5364\n",
      "Epoch 9 Batch 550 Loss 2.4260 Accuracy 0.5372\n",
      "Epoch 9 Batch 600 Loss 2.4239 Accuracy 0.5377\n",
      "Epoch 9 Batch 650 Loss 2.4205 Accuracy 0.5384\n",
      "Epoch 9 Batch 700 Loss 2.4177 Accuracy 0.5390\n",
      "Epoch 9 Loss 2.4180 Accuracy 0.5389\n",
      "Time taken for 1 epoch: 85.66 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.2650 Accuracy 0.5700\n",
      "Epoch 10 Batch 50 Loss 2.2562 Accuracy 0.5585\n",
      "Epoch 10 Batch 100 Loss 2.2602 Accuracy 0.5590\n",
      "Epoch 10 Batch 150 Loss 2.2462 Accuracy 0.5612\n",
      "Epoch 10 Batch 200 Loss 2.2483 Accuracy 0.5604\n",
      "Epoch 10 Batch 250 Loss 2.2455 Accuracy 0.5616\n",
      "Epoch 10 Batch 300 Loss 2.2530 Accuracy 0.5601\n",
      "Epoch 10 Batch 350 Loss 2.2525 Accuracy 0.5606\n",
      "Epoch 10 Batch 400 Loss 2.2526 Accuracy 0.5609\n",
      "Epoch 10 Batch 450 Loss 2.2527 Accuracy 0.5610\n",
      "Epoch 10 Batch 500 Loss 2.2512 Accuracy 0.5615\n",
      "Epoch 10 Batch 550 Loss 2.2520 Accuracy 0.5614\n",
      "Epoch 10 Batch 600 Loss 2.2514 Accuracy 0.5619\n",
      "Epoch 10 Batch 650 Loss 2.2529 Accuracy 0.5618\n",
      "Epoch 10 Batch 700 Loss 2.2519 Accuracy 0.5621\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train\\ckpt-2\n",
      "Epoch 10 Loss 2.2522 Accuracy 0.5620\n",
      "Time taken for 1 epoch: 86.08 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.9655 Accuracy 0.5972\n",
      "Epoch 11 Batch 50 Loss 2.1149 Accuracy 0.5800\n",
      "Epoch 11 Batch 100 Loss 2.1148 Accuracy 0.5808\n",
      "Epoch 11 Batch 150 Loss 2.1149 Accuracy 0.5802\n",
      "Epoch 11 Batch 200 Loss 2.1144 Accuracy 0.5804\n",
      "Epoch 11 Batch 250 Loss 2.1125 Accuracy 0.5806\n",
      "Epoch 11 Batch 300 Loss 2.1149 Accuracy 0.5802\n",
      "Epoch 11 Batch 350 Loss 2.1196 Accuracy 0.5798\n",
      "Epoch 11 Batch 400 Loss 2.1230 Accuracy 0.5793\n",
      "Epoch 11 Batch 450 Loss 2.1193 Accuracy 0.5801\n",
      "Epoch 11 Batch 500 Loss 2.1214 Accuracy 0.5799\n",
      "Epoch 11 Batch 550 Loss 2.1242 Accuracy 0.5797\n",
      "Epoch 11 Batch 600 Loss 2.1258 Accuracy 0.5795\n",
      "Epoch 11 Batch 650 Loss 2.1256 Accuracy 0.5797\n",
      "Epoch 11 Loss 2.1260 Accuracy 0.5798\n",
      "Time taken for 1 epoch: 84.87 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 2.0689 Accuracy 0.5958\n",
      "Epoch 12 Batch 50 Loss 1.9987 Accuracy 0.5957\n",
      "Epoch 12 Batch 100 Loss 1.9908 Accuracy 0.5982\n",
      "Epoch 12 Batch 150 Loss 1.9831 Accuracy 0.5986\n",
      "Epoch 12 Batch 200 Loss 1.9925 Accuracy 0.5973\n",
      "Epoch 12 Batch 250 Loss 1.9967 Accuracy 0.5970\n",
      "Epoch 12 Batch 300 Loss 2.0028 Accuracy 0.5965\n",
      "Epoch 12 Batch 350 Loss 2.0029 Accuracy 0.5963\n",
      "Epoch 12 Batch 400 Loss 2.0010 Accuracy 0.5965\n",
      "Epoch 12 Batch 450 Loss 2.0023 Accuracy 0.5966\n",
      "Epoch 12 Batch 500 Loss 2.0056 Accuracy 0.5960\n",
      "Epoch 12 Batch 550 Loss 2.0094 Accuracy 0.5954\n",
      "Epoch 12 Batch 600 Loss 2.0096 Accuracy 0.5956\n",
      "Epoch 12 Batch 650 Loss 2.0123 Accuracy 0.5952\n",
      "Epoch 12 Batch 700 Loss 2.0139 Accuracy 0.5951\n",
      "Epoch 12 Loss 2.0139 Accuracy 0.5950\n",
      "Time taken for 1 epoch: 85.59 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 2.0517 Accuracy 0.6030\n",
      "Epoch 13 Batch 50 Loss 1.9071 Accuracy 0.6093\n",
      "Epoch 13 Batch 100 Loss 1.9128 Accuracy 0.6078\n",
      "Epoch 13 Batch 150 Loss 1.9114 Accuracy 0.6084\n",
      "Epoch 13 Batch 200 Loss 1.9049 Accuracy 0.6102\n",
      "Epoch 13 Batch 250 Loss 1.9094 Accuracy 0.6100\n",
      "Epoch 13 Batch 300 Loss 1.9116 Accuracy 0.6097\n",
      "Epoch 13 Batch 350 Loss 1.9114 Accuracy 0.6097\n",
      "Epoch 13 Batch 400 Loss 1.9144 Accuracy 0.6095\n",
      "Epoch 13 Batch 450 Loss 1.9151 Accuracy 0.6095\n",
      "Epoch 13 Batch 500 Loss 1.9163 Accuracy 0.6094\n",
      "Epoch 13 Batch 550 Loss 1.9176 Accuracy 0.6093\n",
      "Epoch 13 Batch 600 Loss 1.9206 Accuracy 0.6091\n",
      "Epoch 13 Batch 650 Loss 1.9232 Accuracy 0.6089\n",
      "Epoch 13 Batch 700 Loss 1.9251 Accuracy 0.6085\n",
      "Epoch 13 Loss 1.9251 Accuracy 0.6085\n",
      "Time taken for 1 epoch: 84.69 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.8899 Accuracy 0.6149\n",
      "Epoch 14 Batch 50 Loss 1.8017 Accuracy 0.6256\n",
      "Epoch 14 Batch 100 Loss 1.8154 Accuracy 0.6232\n",
      "Epoch 14 Batch 150 Loss 1.8156 Accuracy 0.6234\n",
      "Epoch 14 Batch 200 Loss 1.8177 Accuracy 0.6231\n",
      "Epoch 14 Batch 250 Loss 1.8217 Accuracy 0.6223\n",
      "Epoch 14 Batch 300 Loss 1.8250 Accuracy 0.6223\n",
      "Epoch 14 Batch 350 Loss 1.8271 Accuracy 0.6220\n",
      "Epoch 14 Batch 400 Loss 1.8323 Accuracy 0.6212\n",
      "Epoch 14 Batch 450 Loss 1.8359 Accuracy 0.6208\n",
      "Epoch 14 Batch 500 Loss 1.8388 Accuracy 0.6206\n",
      "Epoch 14 Batch 550 Loss 1.8431 Accuracy 0.6200\n",
      "Epoch 14 Batch 600 Loss 1.8446 Accuracy 0.6199\n",
      "Epoch 14 Batch 650 Loss 1.8449 Accuracy 0.6199\n",
      "Epoch 14 Batch 700 Loss 1.8487 Accuracy 0.6195\n",
      "Epoch 14 Loss 1.8487 Accuracy 0.6195\n",
      "Time taken for 1 epoch: 85.11 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.9433 Accuracy 0.5967\n",
      "Epoch 15 Batch 50 Loss 1.7706 Accuracy 0.6292\n",
      "Epoch 15 Batch 100 Loss 1.7476 Accuracy 0.6330\n",
      "Epoch 15 Batch 150 Loss 1.7492 Accuracy 0.6329\n",
      "Epoch 15 Batch 200 Loss 1.7455 Accuracy 0.6337\n",
      "Epoch 15 Batch 250 Loss 1.7523 Accuracy 0.6323\n",
      "Epoch 15 Batch 300 Loss 1.7533 Accuracy 0.6328\n",
      "Epoch 15 Batch 350 Loss 1.7569 Accuracy 0.6324\n",
      "Epoch 15 Batch 400 Loss 1.7626 Accuracy 0.6315\n",
      "Epoch 15 Batch 450 Loss 1.7668 Accuracy 0.6312\n",
      "Epoch 15 Batch 500 Loss 1.7685 Accuracy 0.6308\n",
      "Epoch 15 Batch 550 Loss 1.7705 Accuracy 0.6304\n",
      "Epoch 15 Batch 600 Loss 1.7729 Accuracy 0.6301\n",
      "Epoch 15 Batch 650 Loss 1.7765 Accuracy 0.6296\n",
      "Epoch 15 Batch 700 Loss 1.7804 Accuracy 0.6292\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train\\ckpt-3\n",
      "Epoch 15 Loss 1.7805 Accuracy 0.6292\n",
      "Time taken for 1 epoch: 85.40 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.8119 Accuracy 0.6391\n",
      "Epoch 16 Batch 50 Loss 1.6730 Accuracy 0.6459\n",
      "Epoch 16 Batch 100 Loss 1.6830 Accuracy 0.6436\n",
      "Epoch 16 Batch 150 Loss 1.6777 Accuracy 0.6448\n",
      "Epoch 16 Batch 200 Loss 1.6832 Accuracy 0.6446\n",
      "Epoch 16 Batch 250 Loss 1.6855 Accuracy 0.6439\n",
      "Epoch 16 Batch 300 Loss 1.6933 Accuracy 0.6427\n",
      "Epoch 16 Batch 350 Loss 1.6956 Accuracy 0.6423\n",
      "Epoch 16 Batch 400 Loss 1.6982 Accuracy 0.6418\n",
      "Epoch 16 Batch 450 Loss 1.7038 Accuracy 0.6412\n",
      "Epoch 16 Batch 500 Loss 1.7062 Accuracy 0.6407\n",
      "Epoch 16 Batch 550 Loss 1.7077 Accuracy 0.6405\n",
      "Epoch 16 Batch 600 Loss 1.7122 Accuracy 0.6401\n",
      "Epoch 16 Batch 650 Loss 1.7168 Accuracy 0.6393\n",
      "Epoch 16 Batch 700 Loss 1.7183 Accuracy 0.6391\n",
      "Epoch 16 Loss 1.7179 Accuracy 0.6391\n",
      "Time taken for 1 epoch: 85.37 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.5419 Accuracy 0.6578\n",
      "Epoch 17 Batch 50 Loss 1.6523 Accuracy 0.6459\n",
      "Epoch 17 Batch 100 Loss 1.6307 Accuracy 0.6513\n",
      "Epoch 17 Batch 150 Loss 1.6312 Accuracy 0.6511\n",
      "Epoch 17 Batch 200 Loss 1.6384 Accuracy 0.6505\n",
      "Epoch 17 Batch 250 Loss 1.6372 Accuracy 0.6506\n",
      "Epoch 17 Batch 300 Loss 1.6431 Accuracy 0.6496\n",
      "Epoch 17 Batch 350 Loss 1.6456 Accuracy 0.6493\n",
      "Epoch 17 Batch 400 Loss 1.6497 Accuracy 0.6487\n",
      "Epoch 17 Batch 450 Loss 1.6501 Accuracy 0.6488\n",
      "Epoch 17 Batch 500 Loss 1.6507 Accuracy 0.6487\n",
      "Epoch 17 Batch 550 Loss 1.6537 Accuracy 0.6485\n",
      "Epoch 17 Batch 600 Loss 1.6573 Accuracy 0.6480\n",
      "Epoch 17 Batch 650 Loss 1.6617 Accuracy 0.6473\n",
      "Epoch 17 Loss 1.6627 Accuracy 0.6473\n",
      "Time taken for 1 epoch: 85.26 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.5346 Accuracy 0.6680\n",
      "Epoch 18 Batch 50 Loss 1.5672 Accuracy 0.6607\n",
      "Epoch 18 Batch 100 Loss 1.5639 Accuracy 0.6618\n",
      "Epoch 18 Batch 150 Loss 1.5732 Accuracy 0.6609\n",
      "Epoch 18 Batch 200 Loss 1.5730 Accuracy 0.6607\n",
      "Epoch 18 Batch 250 Loss 1.5779 Accuracy 0.6602\n",
      "Epoch 18 Batch 300 Loss 1.5833 Accuracy 0.6596\n",
      "Epoch 18 Batch 350 Loss 1.5870 Accuracy 0.6590\n",
      "Epoch 18 Batch 400 Loss 1.5902 Accuracy 0.6586\n",
      "Epoch 18 Batch 450 Loss 1.5928 Accuracy 0.6580\n",
      "Epoch 18 Batch 500 Loss 1.5988 Accuracy 0.6572\n",
      "Epoch 18 Batch 550 Loss 1.6036 Accuracy 0.6564\n",
      "Epoch 18 Batch 600 Loss 1.6069 Accuracy 0.6559\n",
      "Epoch 18 Batch 650 Loss 1.6127 Accuracy 0.6550\n",
      "Epoch 18 Batch 700 Loss 1.6179 Accuracy 0.6543\n",
      "Epoch 18 Loss 1.6177 Accuracy 0.6544\n",
      "Time taken for 1 epoch: 85.41 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.4325 Accuracy 0.6771\n",
      "Epoch 19 Batch 50 Loss 1.5172 Accuracy 0.6683\n",
      "Epoch 19 Batch 100 Loss 1.5188 Accuracy 0.6681\n",
      "Epoch 19 Batch 150 Loss 1.5218 Accuracy 0.6680\n",
      "Epoch 19 Batch 200 Loss 1.5316 Accuracy 0.6666\n",
      "Epoch 19 Batch 250 Loss 1.5343 Accuracy 0.6667\n",
      "Epoch 19 Batch 300 Loss 1.5403 Accuracy 0.6657\n",
      "Epoch 19 Batch 350 Loss 1.5465 Accuracy 0.6645\n",
      "Epoch 19 Batch 400 Loss 1.5512 Accuracy 0.6637\n",
      "Epoch 19 Batch 450 Loss 1.5574 Accuracy 0.6627\n",
      "Epoch 19 Batch 500 Loss 1.5602 Accuracy 0.6624\n",
      "Epoch 19 Batch 550 Loss 1.5627 Accuracy 0.6621\n",
      "Epoch 19 Batch 600 Loss 1.5668 Accuracy 0.6618\n",
      "Epoch 19 Batch 650 Loss 1.5691 Accuracy 0.6615\n",
      "Epoch 19 Loss 1.5726 Accuracy 0.6611\n",
      "Time taken for 1 epoch: 85.61 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.4573 Accuracy 0.6846\n",
      "Epoch 20 Batch 50 Loss 1.4695 Accuracy 0.6764\n",
      "Epoch 20 Batch 100 Loss 1.4856 Accuracy 0.6740\n",
      "Epoch 20 Batch 150 Loss 1.4929 Accuracy 0.6727\n",
      "Epoch 20 Batch 200 Loss 1.5046 Accuracy 0.6709\n",
      "Epoch 20 Batch 250 Loss 1.5030 Accuracy 0.6712\n",
      "Epoch 20 Batch 300 Loss 1.5108 Accuracy 0.6700\n",
      "Epoch 20 Batch 350 Loss 1.5128 Accuracy 0.6698\n",
      "Epoch 20 Batch 400 Loss 1.5172 Accuracy 0.6691\n",
      "Epoch 20 Batch 450 Loss 1.5211 Accuracy 0.6687\n",
      "Epoch 20 Batch 500 Loss 1.5230 Accuracy 0.6684\n",
      "Epoch 20 Batch 550 Loss 1.5275 Accuracy 0.6676\n",
      "Epoch 20 Batch 600 Loss 1.5302 Accuracy 0.6671\n",
      "Epoch 20 Batch 650 Loss 1.5314 Accuracy 0.6672\n",
      "Epoch 20 Batch 700 Loss 1.5351 Accuracy 0.6666\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train\\ckpt-4\n",
      "Epoch 20 Loss 1.5357 Accuracy 0.6665\n",
      "Time taken for 1 epoch: 86.08 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "\n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
    "    train_step(inp, tar)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a586d8c",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df1e5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  def __init__(self, tokenizers, transformer):\n",
    "    self.tokenizers = tokenizers\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "    # input sentence is portuguese, hence adding the start and end token\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence\n",
    "\n",
    "    # As the output language is english, initialize the output with the\n",
    "    # english start token.\n",
    "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "    start = start_end[0][tf.newaxis]\n",
    "    end = start_end[1][tf.newaxis]\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a python list) so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())\n",
    "      predictions, _ = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # select the last token from the seq_len dimension\n",
    "      predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size) # Why we select output from the last token? \n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # concatentate the predicted_id to the output which is given to the decoder\n",
    "      # as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # output.shape (1, tokens)\n",
    "    text = tokenizers.en.detokenize(output)[0]  # shape: ()\n",
    "\n",
    "    tokens = tokenizers.en.lookup(output)[0]\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop. So recalculate them outside\n",
    "    # the loop.\n",
    "    _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "\n",
    "    return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "924246b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "translater=Translator(tokenizers=tokenizers,transformer=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7186ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Hello, my name is Akash.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc201258",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Python Projects\\NLP_TRANSLATION_WITH_BERT_FROM_SCRATCH\\BERT.ipynb Cell 109\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Python%20Projects/NLP_TRANSLATION_WITH_BERT_FROM_SCRATCH/BERT.ipynb#Y222sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text,_,_\u001b[39m=\u001b[39mtranslater(sentence)\n",
      "\u001b[1;32me:\\Python Projects\\NLP_TRANSLATION_WITH_BERT_FROM_SCRATCH\\BERT.ipynb Cell 109\u001b[0m in \u001b[0;36mTranslator.__call__\u001b[1;34m(self, sentence, max_length)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Python%20Projects/NLP_TRANSLATION_WITH_BERT_FROM_SCRATCH/BERT.ipynb#Y222sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, sentence, max_length\u001b[39m=\u001b[39mMAX_TOKENS):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Python%20Projects/NLP_TRANSLATION_WITH_BERT_FROM_SCRATCH/BERT.ipynb#Y222sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   \u001b[39m# input sentence is portuguese, hence adding the start and end token\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Python%20Projects/NLP_TRANSLATION_WITH_BERT_FROM_SCRATCH/BERT.ipynb#Y222sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(sentence, tf\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Python%20Projects/NLP_TRANSLATION_WITH_BERT_FROM_SCRATCH/BERT.ipynb#Y222sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sentence\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python%20Projects/NLP_TRANSLATION_WITH_BERT_FROM_SCRATCH/BERT.ipynb#Y222sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     sentence \u001b[39m=\u001b[39m sentence[tf\u001b[39m.\u001b[39mnewaxis]\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text,_,_=translater(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e9e4ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Meu nome é Akash. Eu moro na coreia. Eu estudo engenharia na Universidade de Inha.'\n",
    "ground_truth = 'this is a problem we have to solve .'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translater(\n",
    "    tf.constant(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "57215dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'my name is akas . i moro in korea . i study engineering at the university of inha .'>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f2440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "28351a7ac9c5c3f87d7e2cfd5b1f967c228653fc23bb7a0997ca4e9b6c4c6b30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
